{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "\n",
    "学习目标\n",
    "- 学习语言模型，以及如何训练一个语言模型\n",
    "- 学习torchtext的基本使用方法\n",
    "    - 构建 vocabulary\n",
    "    - word to inde 和 index to word\n",
    "- 学习torch.nn的一些基本模型\n",
    "    - Linear\n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU\n",
    "- RNN的训练技巧\n",
    "    - Gradient Clipping\n",
    "- 如何保存和读取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会使用 [torchtext](https://github.com/pytorch/text) 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EMBEDDING_SIZE = 650\n",
    "MAX_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们会继续使用上次的text8作为我们的训练，验证和测试数据\n",
    "- torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集\n",
    "- BPTTIterator可以连续地得到连贯的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 50002\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(lower=True)\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path=\".\", \n",
    "    train=\"text8.train.txt\", validation=\"text8.dev.txt\", test=\"text8.test.txt\", text_field=TEXT)\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "print(\"vocabulary size: {}\".format(len(TEXT.vocab)))\n",
    "\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=device, bptt_len=32, repeat=False, shuffle=True)\n",
    "# bptt_len 回传长度/单词数(seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50000+2(unk,pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['of']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "batch.text.shape # bptt_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,1].data]))#第2个sequence(句子)\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,1].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    batch = next(it)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,2].data]))\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "\n",
    "- 继承nn.Module\n",
    "- __init__函数\n",
    "- forward函数\n",
    "- 其余可以根据模型需要定义相关的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\" 一个简单的循环神经网络\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        ''' 该模型包含以下几层:\n",
    "            - 一个词嵌入层\n",
    "            - 一个循环神经网络层(RNN, LSTM, GRU)\n",
    "            - 一个线性层, 从hidden state到输出单词表, 不需要激活函数\n",
    "            - 一个dropout层, 用来做regularization\n",
    "            - ntoken = vocab_size\n",
    "            - ninp = embedding_size\n",
    "            - nhid = hidden_size\n",
    "        '''\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        \n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            # getattr: 在nn中检索对应rnn_type\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "            \n",
    "        self.decoder = nn.Linear(nhid, ntoken) # hidden_size -> vocab_size\n",
    "        self.init_weights()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, hidden): #hidden: hidden state\n",
    "        ''' Forward pass:\n",
    "            - word embedding\n",
    "            - 输入循环神经网络\n",
    "            - 一个线性层从hidden state转化为输出单词表\n",
    "            - input(text): seq_len * batch_size(torchtext setting) # seq_len = bptt_len\n",
    "        '''\n",
    "        emb = self.drop(self.encoder(input)) # seq_len * batch_size * embed_size\n",
    "        output, hidden = self.rnn(emb, hidden) # hidden:(n_layers * batch_size * hidden_size, n_layers * batch_size * hidden_size) 1: 1layer, 1 direction\n",
    "        output = self.drop(output) # seq_len * batch_size * hidden_size\n",
    "        #decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        #转成二维，便于decoder linear操作 (seq_len * batch_size) * hidden_size\n",
    "        decoded = self.decoder(output.view(-1, output.size(2))) # (seq_len * batch_size) * vocab_size\n",
    "        #转回三维，seq_len * batch_size * vocab_size(其实没必要)\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(-1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz, requires_grad=True): # bsz:batch_size;requires_grad要不要bp到hidden state\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM': # LSTM: h & c, 2 hidden states\n",
    "            return (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))\n",
    "        else:\n",
    "            return weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, n_layers, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们首先定义评估模型的代码。\n",
    "- 模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass/grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_count = 0.\n",
    "    it = iter(data)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            with torch.no_grad():\n",
    "                output, hidden = model(data, hidden)\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            total_count += np.multiply(*data.size())\n",
    "            total_loss += loss.item()*np.multiply(*data.size())\n",
    "            \n",
    "    loss = total_loss / total_count\n",
    "    # 切回train，进入下一个iteration\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this part\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach() # h跟之前节点的联系切断\n",
    "    else: # multiple hidden states\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_iter)\n",
    "hidden = model.init_hidden(BATCH_SIZE)\n",
    "hidden = repackage_hidden(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 650])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape # n_layers * batch_size * nhid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text.shape #seq_len * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 650])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = batch.text\n",
    "data = data.cuda()\n",
    "emb = model.encoder(data)\n",
    "emb.shape #seq_len * batch_size * embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, (hidden, cell) = model.rnn(emb, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 650])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape # seq_len * batch_size * embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50, 650])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape # n_layers * batch_size * embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 650])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.view(-1, output.size(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 50002])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = model.decoder(output.view(-1, output.size(2)))\n",
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 50002])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.view(output.size(0), output.size(1), decoded.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1600, 50002])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.view(-1, VOCAB_SIZE).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss function和optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型：\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用output和target计算cross entropy loss\n",
    "- 清空模型当前gradient\n",
    "- backward pass\n",
    "- gradient clipping，防止梯度爆炸\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 10.822037696838379\n",
      "best model, val loss:  10.822260043438908\n",
      "epoch 0 iter 1000 loss 10.825000762939453\n",
      "epoch 0 iter 2000 loss 10.824640274047852\n",
      "epoch 0 iter 3000 loss 10.822461128234863\n",
      "epoch 0 iter 4000 loss 10.82335376739502\n",
      "epoch 0 iter 5000 loss 10.825164794921875\n",
      "epoch 0 iter 6000 loss 10.818838119506836\n",
      "epoch 0 iter 7000 loss 10.82194709777832\n",
      "epoch 0 iter 8000 loss 10.821283340454102\n",
      "epoch 0 iter 9000 loss 10.823173522949219\n",
      "epoch 0 iter 10000 loss 10.822760581970215\n",
      "epoch 0 iter 11000 loss 6.590358734130859\n",
      "epoch 0 iter 12000 loss 6.119932174682617\n",
      "epoch 0 iter 13000 loss 5.953525543212891\n",
      "epoch 0 iter 14000 loss 5.9914422035217285\n",
      "epoch 1 iter 0 loss 6.077768325805664\n",
      "best model, val loss:  5.618270756366516\n",
      "epoch 1 iter 1000 loss 5.9252214431762695\n",
      "epoch 1 iter 2000 loss 5.803649425506592\n",
      "epoch 1 iter 3000 loss 5.932034969329834\n",
      "epoch 1 iter 4000 loss 5.37612771987915\n",
      "epoch 1 iter 5000 loss 5.721917629241943\n",
      "epoch 1 iter 6000 loss 5.6660261154174805\n",
      "epoch 1 iter 7000 loss 5.618030071258545\n",
      "epoch 1 iter 8000 loss 5.791095733642578\n",
      "epoch 1 iter 9000 loss 5.229241847991943\n",
      "epoch 1 iter 10000 loss 5.848897933959961\n",
      "best model, val loss:  5.123356109586119\n",
      "epoch 1 iter 11000 loss 5.606627941131592\n",
      "epoch 1 iter 12000 loss 5.386953353881836\n",
      "epoch 1 iter 13000 loss 5.276987075805664\n",
      "epoch 1 iter 14000 loss 5.46589469909668\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "val_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden) #hidden state传遍该epoch所有iteration\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1)) #2维-1维\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)#grad clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "    \n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            \n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model, val loss: \", val_loss)\n",
    "                torch.save(model.state_dict(), \"lm-best.th\")\n",
    "            else:\n",
    "                # learning rate decay\n",
    "                scheduler.step()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    best_model = best_model.cuda()\n",
    "best_model.load_state_dict(torch.load(\"lm-best.th\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的模型在valid数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = evaluate(best_model, val_iter)\n",
    "print(\"perplexity: \", np.exp(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的模型在测试数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(best_model, test_iter)\n",
    "print(\"perplexity: \", np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的模型生成一些句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = best_model.init_hidden(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):#依次预测下一个单词\n",
    "    output, hidden = best_model(input, hidden)#output: 1*1*50002\n",
    "    word_weights = output.squeeze().exp().cpu()#squeeze:去除所有维度=1的维度 exp:output分布更加极端(softmax)\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]#也可以直接softmax->argmax返回预测结果\n",
    "    input.fill_(word_idx)#append\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(\" \".join(words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wentao",
   "language": "python",
   "name": "wentao"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
